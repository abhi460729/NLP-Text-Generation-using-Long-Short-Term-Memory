{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "oZNHqp5lnxz7",
    "outputId": "40ed06a4-7cf9-47d4-cc69-a7b7ec2e2205"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-03-15 07:42:05--  https://www.gutenberg.org/files/1661/1661-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 607788 (594K) [text/plain]\n",
      "Saving to: ‘book.txt’\n",
      "\n",
      "book.txt            100%[===================>] 593.54K   498KB/s    in 1.2s    \n",
      "\n",
      "2020-03-15 07:42:07 (498 KB/s) - ‘book.txt’ saved [607788/607788]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#download the data and save it to book.txt\n",
    "!wget https://www.gutenberg.org/files/1661/1661-0.txt -O book.txt\n",
    "\n",
    "#read the file in text string\n",
    "text = open('book.txt', 'r', encoding='utf-8').read()\n",
    "text = text.lower()\n",
    "\n",
    "#create list of sentences\n",
    "sentences = text.split('\\n')\n",
    "\n",
    "#import dependencies to preprocess the text data and making sequences\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#initialize the tokenizer, which can work char by char\n",
    "tokenizer = Tokenizer(oov_token='<UNK>')\n",
    "\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size\n",
    "\n",
    "input_sequences = []\n",
    "for sequence in sequences:\n",
    "  for i in range(1, len(sequence)):\n",
    "    n_gram_sequence = sequence[:i+1]\n",
    "    input_sequences.append(n_gram_sequence)\n",
    "\n",
    "print(input_sequences[0], input_sequences[1], input_sequences[2], input_sequences[3])\n",
    "\n",
    "#find the maximum length among sequences\n",
    "max_seq_len = max([len(seq) for seq in input_sequences])\n",
    "max_seq_len\n",
    "\n",
    "#we will keep the last value of the sequence as our target label and all values before that as input to sequence model\n",
    "\n",
    "#pad the sequences to ensure that they are all of same length\n",
    "padded_sequences = pad_sequences(input_sequences, maxlen = max_seq_len)\n",
    "\n",
    "print(padded_sequences[0], padded_sequences[1])\n",
    "\n",
    "import numpy as np\n",
    "padded_sequences = np.array(padded_sequences)\n",
    "\n",
    "print(len(padded_sequences[0]))\n",
    "print(len(padded_sequences[1]))\n",
    "\n",
    "#prepare training sequences and labels\n",
    "x = padded_sequences[:, : -1]\n",
    "labels = padded_sequences[:, -1]\n",
    "\n",
    "labels.shape\n",
    "\n",
    "#to one hot encode the labels\n",
    "y = tf.keras.utils.to_categorical(labels, num_classes=vocab_size)\n",
    "\n",
    "x.shape\n",
    "\n",
    "#x = x.reshape(x.shape[0], x.shape[1], 1)\n",
    "\n",
    "y.shape\n",
    "\n",
    "#import dependencies for defining the model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "#define and compile the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=max_seq_len-1))\n",
    "model.add(Bidirectional(LSTM(256)))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "adam = Adam(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "#initialize the callback for early stopping the training if there is not at least 1% improvement in the accuracy \n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor = 'acc', min_delta=0.01)\n",
    "\n",
    "model.fit(x, y, epochs=50, verbose=1, batch_size=512, callbacks=[es])\n",
    "\n",
    "#Time to become storyteller!\n",
    "seed_text = \"I could not help laughing at the ease with which he explained his process of deduction\"          \n",
    "next_words = 100\n",
    "  \n",
    "for _ in range(next_words):\n",
    "  sequence = tokenizer.texts_to_sequences([seed_text])\n",
    "  padded = pad_sequences(sequence, maxlen=max_seq_len-1)\n",
    "  predicted = model.predict_classes(padded, verbose=0)\n",
    "  output_word = ''\n",
    "  for word, index in tokenizer.word_index.items():\n",
    "    if index == predicted:\n",
    "      output_word = word\n",
    "      break\n",
    "  seed_text += ' ' + output_word\n",
    "print(seed_text)\n",
    "\n",
    "#let's look at how loss and accuracy changed while training\n",
    "import matplotlib.pyplot as plt\n",
    "history = model.history\n",
    "acc = history.history['acc']\n",
    "loss = history.history['loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
    "plt.title('Training accuracy')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='Training Loss')\n",
    "plt.title('Training loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "project_text_generation_final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
